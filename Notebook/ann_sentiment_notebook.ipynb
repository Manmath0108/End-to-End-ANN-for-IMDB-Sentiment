{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c94edb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/manmath/Desktop/MyProjects/End-to-End-ANN-for-IMDB-Sentiment/Notebook\n",
      "Data directory   : /home/manmath/Desktop/MyProjects/End-to-End-ANN-for-IMDB-Sentiment/Notebook/data/imdb\n",
      "Models directory : /home/manmath/Desktop/MyProjects/End-to-End-ANN-for-IMDB-Sentiment/Notebook/models/ann_imdb\n",
      "Scripts directory: /home/manmath/Desktop/MyProjects/End-to-End-ANN-for-IMDB-Sentiment/Notebook/scripts\n",
      "Has HuggingFace datasets library?: False\n",
      "PyTorch version  : 2.5.1+cu121\n",
      "Device           : cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "    HAS_DATASETS = True\n",
    "except Exception:\n",
    "    HAS_DATASETS = False\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "DATA_DIR = ROOT / \"data\" / \"imdb\"\n",
    "MODELS_DIR = ROOT / \"models\" / \"ann_imdb\"\n",
    "SCRIPTS_DIR = ROOT / \"scripts\"\n",
    "\n",
    "for d in (DATA_DIR, MODELS_DIR, SCRIPTS_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Working directory:\", ROOT)\n",
    "print(\"Data directory   :\", DATA_DIR)\n",
    "print(\"Models directory :\", MODELS_DIR)\n",
    "print(\"Scripts directory:\", SCRIPTS_DIR)\n",
    "print(\"Has HuggingFace datasets library?:\", HAS_DATASETS)\n",
    "print(\"PyTorch version  :\", torch.__version__)\n",
    "print(\"Device           :\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6f07ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets already present.\n",
      "\n",
      "Loading IMDB dataset (max 1000 pos + 1000 neg for train)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d993114652e41948b879f9c4f87e306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229a232f53e14f9e8f369e8eaf8eccca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e81897c2db74cbc99ce807e020750a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3323dbaf9b2c4d8995ae1511cd095599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "plain_text/unsupervised-00000-of-00001.p(…):   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02076730eb3649eb9bdd59fb910bfbd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399b0e71c5ca4533a2aaae0484127528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89629a4c69ff48ce9a9a88d613cfd81c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2000  Test size: 1000\n",
      "\n",
      "--- Sample Positive examples (5) ---\n",
      "- Ok, so it's an adult movie. But it really is very tastefully done. It's obvious that the producers spent a lot of time and money into making a classy sort of movie. I was pleasantly surprised at just \n",
      "- \"Silverlake Life\" is a documentary and it was plain and straightforward. Actually, it was more like a home movie, and if you want dramatic illuminations, see something else. And it's by no means a tea\n",
      "- This early sci-fi masterwork by Herbert George Wells with music by Arthur Bliss is a powerful piece of film-making. Adapted from Wells' somewhat different work by the author, it presents a look at the\n",
      "- Of the three remakes on W. Somerset Maughan's novel, this one is the best one, and not particularly because what John Cromwell brought to the film. The film is worth a look because of the break throug\n",
      "- if you are like me then you will love this great coming of age teen movie.i think it is up there with mischief/book of love/high school USA/shout/calender girl/crybaby/ all great movies set in the lat\n",
      "\n",
      "--- Sample Negative examples (5) ---\n",
      "- Up until this new season I have been a big 'Little Mosque' fan. However, the new season had absolutely RUINED it.<br /><br />The new Christian vicar has destroyed the entire intent of the show. It has\n",
      "- Today, Bea Arthur died so I was cruising around the IMDb Web site and somehow wound up on a show called \"Gloria.\" \"All In The Family\" was a brilliant show for its first four or five years and I bet I \n",
      "- I don't know if this is one of the SyFy Channel original movies, but that's exactly what it feels like. A cheap, low budget action movie that was probably made very quickly, it contains laughable effe\n",
      "- This movie was just plain bad. Just about every cop movie cliché is present and accounted for. Bad guy gets away? check. Partner? check. Wacky personality clash with partner? check. Rookie with someth\n",
      "- i think that new york is a big fake, i mean her whole guidelines of this show is stupid. i enjoyed flavor fl av more better, she acts like a slut, and a hoe put together. her mother is out of this wor\n",
      "\n",
      "Saved small train/test to: data/imdb\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "MAX_PER_LABEL = 1000   \n",
    "\n",
    "\n",
    "try:\n",
    "    from datasets import load_dataset\n",
    "    print(\"datasets already present.\")\n",
    "except Exception:\n",
    "    print(\"datasets not found — installing 'datasets' via pip inside the environment. This may take 20-60s.\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"datasets\"])\n",
    "    from datasets import load_dataset\n",
    "    print(\"datasets installed.\")\n",
    "\n",
    "print(f\"\\nLoading IMDB dataset (max {MAX_PER_LABEL} pos + {MAX_PER_LABEL} neg for train)...\")\n",
    "ds = load_dataset(\"imdb\")\n",
    "train = ds[\"train\"]\n",
    "test = ds[\"test\"]\n",
    "\n",
    "def subsample_by_label(hf_dataset, max_per_label=MAX_PER_LABEL, seed=42):\n",
    "    pos, neg = [], []\n",
    "    for ex in hf_dataset:\n",
    "        if ex[\"label\"] == 1:\n",
    "            pos.append(ex[\"text\"])\n",
    "        else:\n",
    "            neg.append(ex[\"text\"])\n",
    "\n",
    "    random.seed(seed)\n",
    "    random.shuffle(pos)\n",
    "    random.shuffle(neg)\n",
    "\n",
    "    pos = pos[:max_per_label]\n",
    "    neg = neg[:max_per_label]\n",
    "\n",
    "    texts = pos + neg\n",
    "    labels = [1]*len(pos) + [0]*len(neg)\n",
    "    combined = list(zip(texts, labels))\n",
    "    random.shuffle(combined)\n",
    "    texts, labels = zip(*combined)\n",
    "\n",
    "    return list(texts), list(labels)\n",
    "\n",
    "\n",
    "train_texts, train_labels = subsample_by_label(train)\n",
    "test_texts, test_labels = subsample_by_label(test, min(500, MAX_PER_LABEL))\n",
    "\n",
    "print(\"Train size:\", len(train_texts), \" Test size:\", len(test_texts))\n",
    "\n",
    "print(\"\\n--- Sample Positive examples (5) ---\")\n",
    "pos_count = 0\n",
    "for t, l in zip(train_texts, train_labels):\n",
    "    if l == 1:\n",
    "        print(\"-\", t[:200].replace(\"\\n\", \" \"))\n",
    "        pos_count += 1\n",
    "        if pos_count >= 5:\n",
    "            break\n",
    "\n",
    "print(\"\\n--- Sample Negative examples (5) ---\")\n",
    "neg_count = 0\n",
    "for t, l in zip(train_texts, train_labels):\n",
    "    if l == 0:\n",
    "        print(\"-\", t[:200].replace(\"\\n\", \" \"))\n",
    "        neg_count += 1\n",
    "        if neg_count >= 5:\n",
    "            break\n",
    "\n",
    "DATA_DIR = Path(\"data/imdb\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import json\n",
    "with open(DATA_DIR / \"train_small.jsonl\", \"w\", encoding=\"utf8\") as fw:\n",
    "    for txt, lab in zip(train_texts, train_labels):\n",
    "        fw.write(json.dumps({\"text\": txt, \"label\": int(lab)}) + \"\\n\")\n",
    "\n",
    "with open(DATA_DIR / \"test_small.jsonl\", \"w\", encoding=\"utf8\") as fw:\n",
    "    for txt, lab in zip(test_texts, test_labels):\n",
    "        fw.write(json.dumps({\"text\": txt, \"label\": int(lab)}) + \"\\n\")\n",
    "\n",
    "print(\"\\nSaved small train/test to:\", DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc4d4d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8676ff81259d45509f849e566a6f6351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ccdc664c9e4c62aa33a1105b401f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef33a1b82034468c8b151b1cfdfbb373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f6fb28bd7c4d6e98304d048e9275b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "TOKENIZER_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "\n",
    "MAX_LEN = 200  \n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def _init_(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def _len_(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def _getitem_(self, idx):\n",
    "        text = self.texts[idx]\n",
    "\n",
    "        encoded = tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        input_ids = encoded[\"input_ids\"].squeeze(0)      \n",
    "        attention_mask = encoded[\"attention_mask\"].squeeze(0)  \n",
    "        label = torch.tensor(self.labels[idx])\n",
    "\n",
    "        return input_ids, attention_mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5c3dff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch keys: ['input_ids', 'attention_mask', 'labels']\n",
      " input_ids shape: torch.Size([32, 200])\n",
      " attention_mask shape: torch.Size([32, 200])\n",
      " labels shape: torch.Size([32])\n",
      " sample labels (first 8): [1, 0, 0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "import torch\n",
    "\n",
    "MAX_LEN = 200   \n",
    "\n",
    "class IMDBHFDataset(Dataset):\n",
    "    \"\"\"Yields tokenized dicts suitable for HF collators.\"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=MAX_LEN):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        txt = self.texts[idx]\n",
    "        encoded = self.tokenizer(\n",
    "            txt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=False,           \n",
    "            return_attention_mask=True,\n",
    "            return_tensors=None       \n",
    "        )\n",
    "        encoded[\"labels\"] = int(self.labels[idx])\n",
    "        return encoded\n",
    "\n",
    "train_dataset_hf = IMDBHFDataset(train_texts, train_labels, tokenizer, max_length=MAX_LEN)\n",
    "test_dataset_hf  = IMDBHFDataset(test_texts, test_labels, tokenizer, max_length=MAX_LEN)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=\"longest\", return_tensors=\"pt\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_dataset_hf, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collator)\n",
    "test_loader  = DataLoader(test_dataset_hf, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(\"Batch keys:\", list(batch.keys()))\n",
    "print(\" input_ids shape:\", batch[\"input_ids\"].shape)        \n",
    "print(\" attention_mask shape:\", batch[\"attention_mask\"].shape)\n",
    "print(\" labels shape:\", batch[\"labels\"].shape)\n",
    "print(\" sample labels (first 8):\", batch[\"labels\"][:8].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdd512b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286210894203473eb1bcd5cbef563f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe652e7c24c7467caebf1983b33d754f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01  train_loss=0.6934 train_acc=0.5110  val_loss=0.6918 val_acc=0.5130\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b70213310404677bc4c4721df04c6b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b729f04cb9c84b6b8e50601fb2a316b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02  train_loss=0.6904 train_acc=0.5455  val_loss=0.6895 val_acc=0.5790\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63217139d7ca431f95d6f82f7628b00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4182949b4fc48aa9e938d43eeb02fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "eval:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03  train_loss=0.6871 train_acc=0.6005  val_loss=0.6866 val_acc=0.6030\n",
      "Saved model -> models/ann_imdb_simple.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "class SimpleTextANN(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding -> masked mean pooling -> small MLP -> binary logit output\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=128, pad_id=0, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim, padding_idx=pad_id)\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)   \n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        input_ids: [batch, seq_len]\n",
    "        attention_mask: [batch, seq_len] (1 for tokens, 0 for padding)\n",
    "        returns logits: [batch] (raw single logit per example)\n",
    "        \"\"\"\n",
    "\n",
    "        emb = self.embedding(input_ids)\n",
    "\n",
    "        mask = attention_mask.unsqueeze(-1).type(emb.dtype)      \n",
    "        emb_masked = emb * mask                                 \n",
    "        summed = emb_masked.sum(dim=1)                         \n",
    "        lengths = mask.sum(dim=1).clamp(min=1.0)                 \n",
    "        pooled = summed / lengths                                \n",
    "\n",
    "        x = F.relu(self.fc1(pooled))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x).squeeze(-1)  \n",
    "        return logits\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in tqdm(dataloader, desc=\"train\", leave=False):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device).float()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)         \n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * input_ids.size(0)\n",
    "        preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "        correct += (preds == labels.long()).sum().item()\n",
    "        total += input_ids.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    acc = correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "def eval_epoch(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"eval\", leave=False):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device).float()\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            running_loss += loss.item() * input_ids.size(0)\n",
    "            preds = (torch.sigmoid(logits) >= 0.5).long()\n",
    "            correct += (preds == labels.long()).sum().item()\n",
    "            total += input_ids.size(0)\n",
    "\n",
    "    avg_loss = running_loss / total\n",
    "    acc = correct / total\n",
    "    return avg_loss, acc\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "vocab_size = tokenizer.vocab_size if hasattr(tokenizer, \"vocab_size\") else tokenizer.get_vocab_size()\n",
    "pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "\n",
    "model = SimpleTextANN(vocab_size=vocab_size, embed_dim=128, hidden_dim=128, pad_id=pad_id, dropout=0.3).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "EPOCHS = 3   \n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, optimizer, loss_fn, DEVICE)\n",
    "    val_loss, val_acc = eval_epoch(model, test_loader, loss_fn, DEVICE)\n",
    "    print(f\"Epoch {epoch:02d}  train_loss={train_loss:.4f} train_acc={train_acc:.4f}  val_loss={val_loss:.4f} val_acc={val_acc:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"models/ann_imdb_simple.pt\")\n",
    "print(\"Saved model -> models/ann_imdb_simple.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac8ebc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dc2996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
